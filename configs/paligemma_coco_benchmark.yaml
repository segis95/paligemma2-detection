model:
  path: "/path/to/models/paligemma2-10b-mix-448" # "/path/to/models/paligemma2-10b-pt-448"
  use_flash_attention: False

generation:
  do_sample: False
  max_new_tokens: 256
  max_input_length: null # 1200 valid option for 10b-448; may be useful with torch.compile(model, dynamic=False)
  batch_size: 40

coco:
  # A parsed prediction label may contain multiple tokens. This parameter tells how to aggregate their scores:
  # first: (p1, p2, ..., pn) -> p1
  # average: (p1, p2, ..., pn) -> 1/n * \sum_i p_i
  # product: (p1, p2, ..., pn) -> \prod_i p_i
  # product_norm: (p1, p2, ..., pn) -> (\prod_i p_i)^(1/n)
  # max: (p1, p2, ..., pn) -> max(p1, p2, ..., pn)
  # min: (p1, p2, ..., pn) -> min(p1, p2, ..., pn)
  score_aggregation_mode: "product"
  # determines how many of 80 COCO classes are processed in a one PaliGemma call
  # prompt <- '<image>detect class#1 ; class#2 ; ... class#{classes_per_call}\n'
  classes_per_call: 10
  path_to_images: "/path/to/coco/val2017/"
  json_annotations: "/path/to/coco/annotations/annotations/instances_val2017.json"

hydra:
  job:
    chdir: true
  run:
    dir: ./outputs/${oc.env:RUN_TIMESTAMP}

# hydra.mode=MULTIRUN ++coco.classes_per_call=1,2,4,8,16,32,64
# hydra.mode=MULTIRUN ++coco.score_aggregation_mode=first,average,product,product_norm,max,min





